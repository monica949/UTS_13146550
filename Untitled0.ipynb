{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYB6UF350yc6cX26LHYDmk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monica949/UTS_13146550/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Z23THtDbMoW"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class DecisionTreeClassifier:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def _best_split(self, X, y):\n",
        "        # Need at least two elements to split a node.\n",
        "        m = y.size\n",
        "        if m <= 1:\n",
        "            return None, None\n",
        "\n",
        "        # Count of each class in the current node.\n",
        "        num_parent = [np.sum(y == c) for c in range(self.n_classes_)]\n",
        "\n",
        "        # Gini of current node.\n",
        "        best_gini = 1.0 - sum((n / m) ** 2 for n in num_parent)\n",
        "        best_idx, best_thr = None, None\n",
        "\n",
        "        # Loop through all features.\n",
        "        for idx in range(self.n_features_):\n",
        "            # Sort data along selected feature.\n",
        "            thresholds, classes = zip(*sorted(zip(X[:, idx], y)))\n",
        "\n",
        "            # We could actually split the node according to each feature/threshold pair\n",
        "            # and count the resulting population for each class in the children, but\n",
        "            # instead we compute them in an iterative fashion, making this for loop\n",
        "            # linear rather than quadratic.\n",
        "            num_left = [0] * self.n_classes_\n",
        "            num_right = num_parent.copy()\n",
        "            for i in range(1, m):  # possible split positions\n",
        "                c = classes[i - 1]\n",
        "                num_left[int(c)] += 1\n",
        "                num_right[int(c)] -= 1\n",
        "                gini_left = 1.0 - sum(\n",
        "                    (num_left[x] / i) ** 2 for x in range(self.n_classes_)\n",
        "                )\n",
        "                gini_right = 1.0 - sum(\n",
        "                    (num_right[x] / (m - i)) ** 2 for x in range(self.n_classes_)\n",
        "                )\n",
        "\n",
        "                # The Gini impurity of a split is the weighted average of the Gini\n",
        "                # impurity of the children.\n",
        "                gini = (i * gini_left + (m - i) * gini_right) / m\n",
        "\n",
        "                # The following condition is to make sure we don't try to split two\n",
        "                # points with identical values for that feature, as it is impossible\n",
        "                # (both have to end up on the same side of a split).\n",
        "                if thresholds[i] == thresholds[i - 1]:\n",
        "                    continue\n",
        "\n",
        "                if gini < best_gini:\n",
        "                    best_gini = gini\n",
        "                    best_idx = idx\n",
        "                    best_thr = (thresholds[i] + thresholds[i - 1]) / 2  # midpoint\n",
        "\n",
        "        return best_idx, best_thr\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Build decision tree classifier.\"\"\"\n",
        "        self.n_classes_ = len(set(y))  # classes are assumed to go from 0 to n-1\n",
        "        self.n_features_ = X.shape[1]\n",
        "        self.tree_ = self._grow_tree(X, y)\n",
        "\n",
        "    def _grow_tree(self, X, y, depth=0):\n",
        "        \"\"\"Build a decision tree by recursively finding the best split.\"\"\"\n",
        "        # Population for each class in current node. The predicted class is the one with\n",
        "        # largest population.\n",
        "        num_samples_per_class = [np.sum(y == i) for i in range(self.n_classes_)]\n",
        "        predicted_class = np.argmax(num_samples_per_class)\n",
        "        node = Node(\n",
        "            gini=self._gini(y),\n",
        "            num_samples=y.size,\n",
        "            num_samples_per_class=num_samples_per_class,\n",
        "            predicted_class=predicted_class,\n",
        "        )\n",
        "\n",
        "        # Split recursively until maximum depth is reached.\n",
        "        if depth < self.max_depth:\n",
        "            idx, thr = self._best_split(X, y)\n",
        "            if idx is not None:\n",
        "                indices_left = X[:, idx] < thr\n",
        "                X_left, y_left = X[indices_left], y[indices_left]\n",
        "                X_right, y_right = X[~indices_left], y[~indices_left]\n",
        "                node.feature_index = idx\n",
        "                node.threshold = thr\n",
        "                node.left = self._grow_tree(X_left, y_left, depth + 1)\n",
        "                node.right = self._grow_tree(X_right, y_right, depth + 1)\n",
        "        return node\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._predict(inputs) for inputs in X]\n",
        "\n",
        "    def _predict(self, inputs):\n",
        "        node = self.tree_\n",
        "        while node.left:\n",
        "            if inputs[node.feature_index] < node.threshold:\n",
        "                node = node.left\n",
        "            else:\n",
        "                node = node.right\n",
        "        return node.predicted_class\n",
        "\n",
        "    def _gini(self, y):\n",
        "        m = y.size\n",
        "        if m <= 1:\n",
        "            return None\n",
        "        num_parent = [np.sum(y == c) for c in range(self.n_classes_)]\n",
        "        return 1.0 - sum((n / m) ** 2 for n in num_parent)\n",
        "\n"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIG0poxkbvQ5"
      },
      "source": [
        "class Node:\n",
        "    def __init__(self, gini, num_samples, num_samples_per_class, predicted_class):\n",
        "        self.gini = gini\n",
        "        self.num_samples = num_samples\n",
        "        self.num_samples_per_class = num_samples_per_class\n",
        "        self.predicted_class = predicted_class\n",
        "        self.feature_index = 0\n",
        "        self.threshold = 0\n",
        "        self.left = None\n",
        "        self.right = None"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vgtO0RTccWl",
        "outputId": "7302fd93-4895-486c-930c-dcf9abb68c37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load data.\n",
        "url = \"https://raw.githubusercontent.com/monica949/UTS_13146550/main/Iris_Final.csv\"\n",
        "df= pd.read_csv(url)\n",
        "df=df.drop(\"Row\",axis=1)\n",
        "df=df.rename(columns={\"Species\" :\"Label\"})\n",
        "\n",
        "X=df.iloc[:, [0,1,2,3]].values\n",
        "Y=df.iloc[:, [4]].values\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# Algorithm\n",
        "clf = DecisionTreeClassifier(max_depth=2)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Prediction\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "confusionmatrix = confusion_matrix(y_test, y_pred)\n",
        "print(confusionmatrix)\n",
        "print(\"Accuracy of the decision tree classifier is:\", accuracy_score(y_test, y_pred))\n"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 9  0  0]\n",
            " [ 0 11  1]\n",
            " [ 0  0  9]]\n",
            "Accuracy of the decision tree classifier is: 0.9666666666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkQ-r8RIzLSC"
      },
      "source": [
        "def compute_entopy(y):\n",
        "  if len(y) < 2:\n",
        "    return 0\n",
        "  freq = np.array( y.value_counts(normalize = True))\n",
        "  return -(freq * np.log2(freq + 1e-6)).sum()\n",
        "\n",
        "\n",
        "def compute_info_gain(samples , attr, target):\n",
        "\n",
        "  values = samples[attr].value_counts(normalize = True)\n",
        "  split_ent = 0\n",
        "  for v, fr in values.iteritems():\n",
        "    sub_ent = compute_entropy(\n",
        "        samples[samples[attr]==v][target])\n",
        "    split_ent += fr * sub_ent\n",
        "\n",
        "  ent = compute_entropy(samples[target])\n",
        "  return ent - split_ent"
      ],
      "execution_count": 108,
      "outputs": []
    }
  ]
}